Idea：
把数据区分为重要部分和大数据部分。
重要部分含有数据的重要索引分类信息，并且对数据的操作有重要意义。
大数据部分通常是无意义的， 但是和重要数据相关联。这些数据并不需要存储在数据库的快速索引部分，而只需要存储于某个类似于CDN的服务器之上， 通过重要数据的索引进行访问即可。

特色：
让数据库节点可以自己解决重新分配工作问题。重新分配可以分为三步：
需要重新分配的时候，在后台进行准备新的节点
默默地把数据拷贝到新的节点， 拷贝一点，forward一点，保持所有复制节点同步
达到一定规模以后，通知路由器，修改路由

用例：
数据太大， 需要分裂
数据太忙， 需要多个节点共同分担
数据危险性大或者安全要求高， 需要更多的备份

Idea：
自适应的大数据存储方案。可以采用不同的存储方式，引用存储，引用加增量，等等

Data的特性：
大小，访问频度，更新频度，性能要求，复杂性
根据数据的特性，可以进化出不同的存储群，以适应用户的需求

数据的关联：
有些数据必须在一起，或者离得比较近，所以要求一些存储群要物理上保存在较近的位置，便于用户一次同时取出，或者在本地进行计算后取得结果。比较远的节点由于性能差逐渐被淘汰，或者逐渐搬家到近处。

分为灾难性应激和非灾难性进化
一群节点互相比拼，更有效的节点会有机会把基因扩散到其他节点，用神经网络学习来决定两个节点的相似性

network connection:

1. Use udp not tcp to speed up
2. Udp send, wait for udp response, if no response is issued, send again or try another server
3. All udp packages will have a tracking id, if some of them are missing, retry with the id

process management:

There would be thousands of endpoints per host, however there would not be so many cpu cores available for each endopint. Thus we must implement a process management system ourself. We can not use the linux system process or thread system because linux has limits for processes (threads similarly), and thousands of processes will run very slowly in linux.

We will create a cradle process for each cpu core in the system. Each cradle process will have only one socket listener, processing incoming and outgoing messages. Each process will also have a container for thousands of endpoints. An endpoint is an instance in the container. It will have its own stack for partition map, health, qos, and other compulsory items unique to each endpoint. Requests and replies will be handled within process owned containers to save cost. Each endpoint will also take care of its own BDB(possibly some other key value persistent store). When the requests and replies needs to read or write, they will queue some tasks for the store reader or writer, the process will make asyn calls to fetch data from database.

The endpoints should made as small as possible. Micro endpoints make scaling and repartitioning easy and low cost, so that the cluster could make frequent repartitioning as needed to adapt to fast changing request model.

Once it is found a partition goes over size, it will split itself into two, so that two endpoints could share its workload. Once an endpoint gets hot, it will try to replicate itself into some other free hosts, so that its partition get more replicas to serve the requests. Writing hot could be solved by assigning different keys to different endpoints in the same partition, and reading with the same strategy. Reading hot could be solved by caching or creating more replicas that near the requesters. A partition should have different strategies to adapt to different kind of data flow mode.

All endpoints should be able to backup themselves somewhere in the cluster in real time. Each of them will try to copy themselves to some other hosts which have free space. Once the endpoints accidentally died, they should immediately start working on those hosts without any host replacement process.

The endpoints should be able to monitor the host health status. Once the host have more hardware errors, or other sign that it could go bad, it should consider migrating to other hosts.

We need a map from partition to endpoints, and endpoints to host to find where the request goes to. Endpoints only talk to other endpoints in the same partition, they are not aware of any endpoint outside the partition.

To simplify, we create a cradle for each host only for now. Running multiple cradles on the same host will have disk usage competition and network competition problems. We will address these problems in the future. Alternatively, we can just use one cradle for each host, but creating multiple threads to run different set of endpoints. This would be a little bit simpler but still chanllenging.


